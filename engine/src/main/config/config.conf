#Print streams in each spark iteration
printStreams = false

#Enable stats
statsEnabled = false
	
#Save all actions realized on the platform
auditEnabled = false
	
// Clustering configuration. From this section is possible to configure the High Availability and Sharding capabilities
clustering = {

	// Is the clustering enabled? If true, one of the Decision instances which belongs to the same clusterId will be on charge of manage the cluster status and sync operations
	enabled = false

	// The name of the group. A group is composed by the number of Decision instances configured in the same way in order to offer HA capabilities. To the same group can belongs 1 (not HA) or 2 (HA) Decision instances.
	groupId = "default"

	// List of groups which belongs to the same cluster. It should be the same list in all nodes of the cluster.
	#clusterGroups = ["groupA", "groupB"]

	// Only applies if clustering flag is activated.
	// If true, Decision shards the data automatically into differents topics, named as
	// stratio_decision_data_partition_partitionNumber.
	partitionsEnabled = false

	// List of Kafka topics subscribed by this Decisision instance
	#dataTopics = ["topic_A"]

	// Is failover enabled? It have to be true if you need HA capabilities
	failoverEnabled = false

	failoverPeriod = 300s

	allAckEnabled= false

	ackTimeout = 500 //ms
}

kafka = {
	hosts = ["localhost:9092"]
	connectionTimeout = 10000
	sessionTimeout = 10000
	zookeeperPath = ""
	
	# default replication factor and partitions for internal topics
	replicationFactor = 1
	partitions = 1
}

zookeeper = {
	hosts = ["localhost:2181"]
}
spark = {
	internalHost = "local[6]"
	internalStreamingBatchTime = 2 s
	
	host ="local[6]"
	streamingBatchTime = 2 s
}

cassandra = {
	hosts = ["localhost:9042"]
	# max size of elements in the insert batch.
	# Cassandra does not work properly with batches over 50 elements, so we strongly recommend not to modify this value
	maxBatchSize = 50
	# batchType property values allowed: LOGGED, UNLOGGED
	batchType = "UNLOGGED"
}

mongo = {
	hosts = ["localhost:27017"]
	#username = ""
	#password= ""
	# max size of elements in the bulk insert batch.
	# Mongo does not work properly with batches over 1000 elements, so we strongly recommend not to modify this value
	maxBatchSize = 1000
}

elasticsearch = {
	hosts = ["localhost:9300"]
	clusterName = "elasticsearch"
	# max size of elements in the index batch.
	# The proper max batch size for a bulk indexing in elastic search depends on several factors, but a max size of
	# 1000 elements is a sensible default value
	maxBatchSize = 1000
}

solr = {
	host = "localhost:8983"
	cloud = true
	dataDir = "/opt/sds/solr/examples/solr"
}

